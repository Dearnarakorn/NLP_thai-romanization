{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAsMfdos6jD7",
        "outputId": "bfe1cd3c-6dbb-4ecd-c1e2-5ffe0e3ed6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-13 02:47:43--  https://www.dropbox.com/scl/fi/macggi1t14ui1cnooqxdv/NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com/cd/0/inline/CO8Sucgxf9IzI0Q8RVE1J-TB6wv0WSh-cGXiORvbGDdXSnoprvjqYU_U7lTz9DS9kZLYjWT3R1OEdtw37VU2-4AAoxzueeKTap-Pt4NwRQh0zopuVsbxMpN78sBlXC6natv4qWgYVZVNJopjctYc-keo/file# [following]\n",
            "--2024-03-13 02:47:44--  https://ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com/cd/0/inline/CO8Sucgxf9IzI0Q8RVE1J-TB6wv0WSh-cGXiORvbGDdXSnoprvjqYU_U7lTz9DS9kZLYjWT3R1OEdtw37VU2-4AAoxzueeKTap-Pt4NwRQh0zopuVsbxMpN78sBlXC6natv4qWgYVZVNJopjctYc-keo/file\n",
            "Resolving ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com (ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com (ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CO_WMuNB2gISIijCpKpzbmGT17oMi-0OuM0IsIMRv0TC2IEUmt_pvFxXySaHIkjhrBVCni3KDa3UfD2WWCM4cB-cnd1ExcgHP4glMTTnai1T4rL4Q9pxsDse2T-wB9iquORpsNuUfogrNFAWtKKJQDg27YmH4qOmngXF_w93F-wD_fs1qaSiWr6gRA8oy5nsf7wXwWEQqUlvHQ2R96okIgnGkLuALmwU81TQfbwBQmPhUWubFZdLoysBKB3RSKknlzDYDvxZOBE536oIRcgf381JQVBWE1v5GxXQfQT59Ke-G8XfP4xfX2monDL4Bfqw6Few1XM2BZ-DeShAn_oO_825gzdAaHEmvDLIdJjdr0yG1dqfLh0SUP__n2xUCwR41fc/file [following]\n",
            "--2024-03-13 02:47:44--  https://ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com/cd/0/inline2/CO_WMuNB2gISIijCpKpzbmGT17oMi-0OuM0IsIMRv0TC2IEUmt_pvFxXySaHIkjhrBVCni3KDa3UfD2WWCM4cB-cnd1ExcgHP4glMTTnai1T4rL4Q9pxsDse2T-wB9iquORpsNuUfogrNFAWtKKJQDg27YmH4qOmngXF_w93F-wD_fs1qaSiWr6gRA8oy5nsf7wXwWEQqUlvHQ2R96okIgnGkLuALmwU81TQfbwBQmPhUWubFZdLoysBKB3RSKknlzDYDvxZOBE536oIRcgf381JQVBWE1v5GxXQfQT59Ke-G8XfP4xfX2monDL4Bfqw6Few1XM2BZ-DeShAn_oO_825gzdAaHEmvDLIdJjdr0yG1dqfLh0SUP__n2xUCwR41fc/file\n",
            "Reusing existing connection to ucf4b1b8df28b1314167bd3b5197.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13479653 (13M) [application/zip]\n",
            "Saving to: ‘NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby’\n",
            "\n",
            "NLPmaterial.zip?rlk 100%[===================>]  12.85M  50.4MB/s    in 0.3s    \n",
            "\n",
            "2024-03-13 02:47:45 (50.4 MB/s) - ‘NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby’ saved [13479653/13479653]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://www.dropbox.com/scl/fi/macggi1t14ui1cnooqxdv/NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby&dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Yqo6LJ7QtO",
        "outputId": "3032a559-aa7f-45da-e717-0f97cbc8a4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby\n",
            "  inflating: material/input_token_index.json  \n",
            "  inflating: material/s2s_gru_model100.keras  \n",
            "  inflating: material/s2s_lstm_model100.keras  \n",
            "  inflating: material/s2s_lstm_model20.keras  \n",
            "  inflating: material/target_token_index.json  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/NLPmaterial.zip?rlkey=pp713v7s9r6b0j8u718aepuby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_yJgpiJ9VuO",
        "outputId": "646ac7ff-285e-42a2-ae71-13ffa00a551b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.12.0 (from gradio)\n",
            "  Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.12.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.12.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=ec5a6f3644bdc213fa1c0a7175efe2808b1247b7cec83340e13f9bd017ffcc10\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.21.0 gradio-client-0.12.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 orjson-3.9.15 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.28.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73MC9PRG8Jb7"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import json\n",
        "import numpy as np\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEQjmmlx8XBo"
      },
      "outputs": [],
      "source": [
        "modelLSTM100 = keras.models.load_model(\"/content/material/s2s_lstm_model100.keras\") #50000 simples\n",
        "modelLSTM20 = keras.models.load_model(\"/content/material/s2s_lstm_model20.keras\") #100000 simples\n",
        "modelGRU = keras.models.load_model(\"/content/material/s2s_gru_model100.keras\") #50000 simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKWHvYM88hka"
      },
      "outputs": [],
      "source": [
        "num_decoder_tokens = 36\n",
        "num_encoder_tokens = 85\n",
        "latent_dim = 256\n",
        "max_decoder_seq_length = 43\n",
        "max_encoder_seq_length = 29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntsf2HPo8iVv"
      },
      "outputs": [],
      "source": [
        "with open('/content/material/target_token_index.json', 'r') as json_file:\n",
        "    target_token_index = json.load(json_file)\n",
        "\n",
        "with open('/content/material/input_token_index.json', 'r') as json_file:\n",
        "    input_token_index = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvEVHdYJ8xwF"
      },
      "outputs": [],
      "source": [
        "encoder_inputs_lstm = modelLSTM100.input[0]  # input_1\n",
        "encoder_outputs__lstm, state_h_enc_lstm, state_c_enc_lstm = modelLSTM100.layers[2].output  # lstm_1\n",
        "encoder_states__lstm = [state_h_enc_lstm, state_c_enc_lstm]\n",
        "encoder_model__lstm = keras.Model(encoder_inputs_lstm, encoder_states__lstm)\n",
        "\n",
        "decoder_inputs_lstm = modelLSTM100.input[1]  # input_2\n",
        "decoder_state_input_h_lstm = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c__lstm = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs_lstm = [decoder_state_input_h_lstm, decoder_state_input_c__lstm]\n",
        "decoder_lstm = modelLSTM100.layers[3]\n",
        "decoder_outputs_lstm, state_h_dec_lstm, state_c_dec_lstm = decoder_lstm(\n",
        "    decoder_inputs_lstm, initial_state=decoder_states_inputs_lstm\n",
        ")\n",
        "decoder_states_lstm = [state_h_dec_lstm, state_c_dec_lstm]\n",
        "decoder_dense_lstm = modelLSTM100.layers[4]\n",
        "decoder_outputs_lstm = decoder_dense_lstm(decoder_outputs_lstm)\n",
        "decoder_model_lstm = keras.Model(\n",
        "    [decoder_inputs_lstm] + decoder_states_inputs_lstm, [decoder_outputs_lstm] + decoder_states_lstm\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-FQNWQvXtz"
      },
      "outputs": [],
      "source": [
        "def decode_sequence_lstm(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model__lstm.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    # print(\"tar \"+ target_seq)\n",
        "    # print(\"sta \"+ states_value)\n",
        "    #print(f\"tar: {target_seq.shape}\")\n",
        "    #print(f\"sta: {states_value}\")\n",
        "    #print(type(states_value))\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model_lstm.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHOsH7kz9HQG"
      },
      "outputs": [],
      "source": [
        "encoder_inputs_lstm2 = modelLSTM20.input[0]  # input_1\n",
        "encoder_outputs__lstm2, state_h_enc_lstm2, state_c_enc_lstm2 = modelLSTM20.layers[2].output  # lstm_2\n",
        "encoder_states__lstm2 = [state_h_enc_lstm2, state_c_enc_lstm2]\n",
        "encoder_model__lstm2 = keras.Model(encoder_inputs_lstm2, encoder_states__lstm2)\n",
        "\n",
        "decoder_inputs_lstm2 = modelLSTM20.input[1]  # input_2\n",
        "decoder_state_input_h_lstm2 = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c__lstm2 = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs_lstm2 = [decoder_state_input_h_lstm2, decoder_state_input_c__lstm2]\n",
        "decoder_lstm2 = modelLSTM20.layers[3]\n",
        "decoder_outputs_lstm2, state_h_dec_lstm2, state_c_dec_lstm2 = decoder_lstm2(\n",
        "    decoder_inputs_lstm2, initial_state=decoder_states_inputs_lstm2\n",
        ")\n",
        "decoder_states_lstm2 = [state_h_dec_lstm2, state_c_dec_lstm2]\n",
        "decoder_dense_lstm2 = modelLSTM20.layers[4]\n",
        "decoder_outputs_lstm2 = decoder_dense_lstm2(decoder_outputs_lstm2)\n",
        "decoder_model_lstm2 = keras.Model(\n",
        "    [decoder_inputs_lstm2] + decoder_states_inputs_lstm2, [decoder_outputs_lstm2] + decoder_states_lstm2\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "# reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hd6oXrr88XD"
      },
      "outputs": [],
      "source": [
        "def decode_sequence_lstm2(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model__lstm2.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    # print(\"tar \"+ target_seq)\n",
        "    # print(\"sta \"+ states_value)\n",
        "    #print(f\"tar: {target_seq.shape}\")\n",
        "    #print(f\"sta: {states_value}\")\n",
        "    #print(type(states_value))\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model_lstm2.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LPQXgV4vh9Z"
      },
      "outputs": [],
      "source": [
        "def encode_input(name):\n",
        "    test_input = np.zeros(\n",
        "        (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype='float32')\n",
        "    for t, char in enumerate(name):\n",
        "        test_input[0, t, input_token_index[char]] = 1.\n",
        "    return test_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjCwshTP9Cx1"
      },
      "outputs": [],
      "source": [
        "encoder_inputs_gru = modelGRU.input[0]  # input_1\n",
        "encoder_outputs_gru, state_h_enc_gru = modelGRU.layers[2].output  # gru_1\n",
        "encoder_states_gru = [state_h_enc_gru]\n",
        "encoder_model_gru = keras.Model(encoder_inputs_gru, encoder_states_gru)\n",
        "\n",
        "decoder_inputs_gru = modelGRU.input[1]  # input_2\n",
        "decoder_state_input_h_gru = keras.Input(shape=(latent_dim,))\n",
        "#decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs_gru = [decoder_state_input_h_gru]\n",
        "decoder_gru = modelGRU.layers[3]\n",
        "decoder_outputs_gru, state_h_dec_gru = decoder_gru(\n",
        "    decoder_inputs_gru, initial_state=decoder_states_inputs_gru\n",
        ")\n",
        "decoder_states_gru = [state_h_dec_gru]\n",
        "decoder_dense_gru = modelGRU.layers[4]\n",
        "decoder_outputs_gru = decoder_dense_gru(decoder_outputs_gru)\n",
        "decoder_model_gru = keras.Model(\n",
        "    [decoder_inputs_gru] + decoder_states_inputs_gru, [decoder_outputs_gru] + decoder_states_gru\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "#reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "#reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efTECWtR9KSw"
      },
      "outputs": [],
      "source": [
        "def decode_sequence_gru(input_seq):\n",
        "    states_value = encoder_model_gru.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h = decoder_model_gru.predict([target_seq] + [np.array(states_value)])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = h\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcq4s7A69N3p"
      },
      "outputs": [],
      "source": [
        "def decode_sequence_gru(input_seq):\n",
        "    states_value = encoder_model_gru.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h = decoder_model_gru.predict([target_seq] + [np.array(states_value)])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "        states_value = h\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzJCzLj4vmcm"
      },
      "outputs": [],
      "source": [
        "def encode_input(name):\n",
        "    test_input = np.zeros(\n",
        "        (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype='float32')\n",
        "    for t, char in enumerate(name):\n",
        "        test_input[0, t, input_token_index[char]] = 1.\n",
        "    return test_input\n",
        "\n",
        "def rom_lstm100(text):\n",
        "\treturn decode_sequence_lstm(encode_input(text))\n",
        "\n",
        "def rom_lstm20(text):\n",
        "\treturn decode_sequence_lstm2(encode_input(text))\n",
        "\n",
        "def rom_gru(text):\n",
        "\treturn decode_sequence_gru(encode_input(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPaXU6Ih93jB",
        "outputId": "ffbf2a7d-34d2-4d6e-be51-b5448927cc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 490ms/step\n",
            "1/1 [==============================] - 0s 493ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 408ms/step\n",
            "1/1 [==============================] - 0s 476ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 388ms/step\n",
            "1/1 [==============================] - 0s 406ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        }
      ],
      "source": [
        "r1 = rom_lstm100('สวัสดีไก่ปลา')\n",
        "r2 = rom_lstm20('สวัสดีไก่ปลา')\n",
        "r3 = rom_gru('สวัสดีไก่ปลา')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUcfN7fovpeX",
        "outputId": "9cda7768-8686-46bb-9e07-51436d721dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lstm100: sawatdiakkilap\n",
            "\n",
            "lstm20: sawattikaipla\n",
            "\n",
            "gru: sawatsikailak\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"lstm100: {r1}\")\n",
        "print(f\"lstm20: {r2}\")\n",
        "print(f\"gru: {r3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "RQOuJGdT9RZO",
        "outputId": "da2611c8-32e5-4969-afa4-efb9bee950c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://21140fd437d1b9186a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://21140fd437d1b9186a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(input):\n",
        "    return f\"LSTM100:{rom_lstm100(input)}\\nLSTM20: {rom_lstm20(input)}\\nGRU: {rom_gru(input)}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            thai = gr.Textbox(label=\"Thai Text\")\n",
        "            translate_btn = gr.Button(value=\"Translate\")\n",
        "        with gr.Column():\n",
        "            roman = gr.Textbox(label=\"Romanrize Text\")\n",
        "    translate_btn.click(greet, inputs=thai, outputs=roman)\n",
        "    examples = gr.Examples(examples=[\"แพลนนิง\",\"นิสัย\",\"ลาก่อน\",\"พรุ่งนี้\"],inputs=thai)\n",
        "# demo = gr.Interface(fn=greet, inputs=gr.Textbox(lines= 2, placeholder='Text here'), outputs=\"textbox\")\n",
        "\n",
        "demo.launch(share=True)  # Share your demo with just 1 extra parameter 🚀"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
